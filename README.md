# Image_Generation_using_Stable_Diffusion

# Part 1: Stable Diffusion
  - Installing the libraries (xformers library to memory optimization)
  - Pipeline for image generation: Creating the prompt -> Generating the image -> Saving the result
  - Generating multiple images
  - Parameters: Seed, Inference steps, Guidance scale (CFG), Image size (dimensions), Negative prompt
  - Other models: SD v1.5, SD v2.x, Fine-tuned models with specific styles
  - Changing the scheduler: PNDM (default), DDIM Scheduler, K-LMS Scheduler, Euler Ancestral Discrete Scheduler (Euler A), DPM Scheduler
# Part 2: Prompt Engineering
  - Exploring the prompts: Subject / object, Action and location, Type, Style, Colors, Artist, Resolution, Site. And Other attributes: Ilumination, Negative prompts
  - Use cases: Generating arts, Generating photographs, Generating landscapes, Generating 3D images, Generating drawings, Generating architectures
  - Improving the results using custom models: Anything (cag/anything-v3-1), DreamShaper (Lykon/DreamShaper), Realistic Vision (SG161222/Realistic_Vision_V1.4), Analog Diffusion (wavymulder/Analog-Diffusion), Protogen (darkstorm2150/Protogen_x3.4_Official_Release), Mitsua Diffusion One (Mitsua/mitsua-diffusion-one)
# Part 3: Fine-tuning
# Part 4: Image-to-image
# Part 5: Inpainting
# Part 6: ControlNet
